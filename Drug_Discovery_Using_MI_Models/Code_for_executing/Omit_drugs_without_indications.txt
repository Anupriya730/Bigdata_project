from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Assuming you already have a SparkSession named 'spark'
# If not, create one using: spark = SparkSession.builder.appName("YourAppName").getOrCreate()

# Assuming 'clean_drug_targets_moa_pathway_indications' is a PySpark DataFrame
# If it's not a DataFrame, you can convert it using: spark.createDataFrame(pandas_df)

# Example data creation, replace this with your actual data loading logic
data = [
    ("A", "actionType1", "target1", "pathway1", "indication1"),
    ("B", "actionType2", "target2", "pathway2", None),
    ("C", "actionType3", "target3", "pathway3", "indication3"),
]
columns = ["chemblIds", "actionType", "targets", "pathwayCategory", "approvedIndications"]
clean_drug_targets_moa_pathway_indications = spark.createDataFrame(data, columns)

# Filter out rows where 'approvedIndications' is not null
clean_drug_targets_moa_pathway_indications = clean_drug_targets_moa_pathway_indications \
    .filter(col("approvedIndications").isNotNull())

# Show the resulting DataFrame
clean_drug_targets_moa_pathway_indications.show()
